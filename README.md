# pythonScrawl
给刚开始学习爬虫白白们
新手入门爬虫篇
对于学习爬虫而言，最重要的是兴趣。兴趣将驱使我们克服种种困难，从小白变成“不小白”就是爬虫界的大神啦。爬虫是获取数据的一种方法，作为大数据的第一环节，今天小编给各位新手们支一些招数，或许会让你茅塞顿开。
第一：为了帮助你尽快入门，你首先需要一本中文的关于爬虫的书。小编在这推荐三本书，按照简单程度排列《Python 网络爬虫实战》、《Python数据抓取技术与实战》、《用Python写网络爬虫》，

爬虫三剑客
首先大概地扫描三本书的目录，并留意重复的实战部分，在由浅入深地学习完三本书的基础知识后，把记录的重复实战部分优先学习，倘若你在编写代码的时候有任何问题，可以网上查询，最后自己找一个类似网站，或者完成书本的实战习题，最后的最后的巩固部分就要通过查询相关库的文档对该知识点进行加深。

爬虫大局观篇
在这三本书中，你一定了解到了一些能帮助你快速开发的框架或模块，比如Scrapy，Beautiful Soup，Selenium等等，掌握它们后将加快你的开发的速度。对于框架和模块的学习，这三本书仅仅是敲门砖，小编认为倘若你想要更全面地了解一些知识，可以配合官方文档，后期懂得基本的使用方法后，好好研读官方文档，很有用。

实践学习篇
在简书和CSND博客上，你经常能看到博主发表的一些好的文章，你可以参考他们最终拿到的数据，然后自己去做试验，看看是否成功地得到数据。经过试验后，回过头对比别人的解决方法，这样不断地反省自己解决问题的思路，调整编码风格。



有没有清晰了一些呢？稍后小编会建立一个微信群供大家学习讨论。更多精彩文章推送，请关注我们的公众号apple0818love，

同时欢迎大家在评论里发表自己想要抓取的数据（比如目标网站，目标数据等），小编会将解决方案回复你们，为你排忧解难，
最后
最后
...
如果您急需数据，可私聊小编哦~~~





以下是对每个项目的简单介绍：

#getNews.py
use python to crawl information from web
通过使用python asyncio 获取新浪最新新闻

#douban
使用scrapy抓取豆瓣top250的图书信息，一下部分评论

#doubanCheck
mechanize模块，运用cookies和user_agent模拟登录豆瓣网
headerRaw.txt存储的是手动登录豆瓣网后，从chrome的network中对主页的请求参数复制过来形成的一个文件
Accept:
Accept-Encoding:
...
Cookie:
...
User-Agent:
getHeader.py是对该headerRaw.txt提取，返回cookies和useragent的元组
